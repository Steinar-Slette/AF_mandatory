{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info\n",
    "\n",
    "This notebook contains all the code used to complete the task and make plots used in the report.\n",
    "\n",
    "This notebook is ordered according to the task numbers. Before the tasks is the setup & imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (Classifiers and fairness considerations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "feature_names = ['AGEP', # Age\n",
    "                 \"CIT\", # Citizenship status\n",
    "                 'COW', # Class of worker\n",
    "                 \"ENG\", # Ability to speak English\n",
    "                 'SCHL', # Educational attainment\n",
    "                 'MAR', # Marital status\n",
    "                 \"HINS1\", # Insurance through a current or former employer or union\n",
    "                 \"HINS2\", # Insurance purchased directly from an insurance company\n",
    "                 \"HINS4\", # Medicaid\n",
    "                 \"RAC1P\", # Recoded detailed race code\n",
    "                 'SEX']\n",
    "\n",
    "target_name = \"PINCP\" # Total person's income\n",
    "\n",
    "def data_processing(data, features, target_name:str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[features + [\"target\", target_name]] ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "    cols = {\n",
    "        \"HINS1\": 'HINS1_2.0',\n",
    "        \"HINS2\": 'HINS2_2.0',\n",
    "        \"HINS4\": 'HINS4_2.0',\n",
    "        \"CIT\": 'CIT_1.0',\n",
    "        \"COW\" : 'COW_1.0',\n",
    "        \"SCHL\" : 'SCHL_16.0',\n",
    "        \"MAR\": 'MAR_5.0',\n",
    "        \"SEX\": 'SEX_1.0',\n",
    "        \"RAC1P\" :'RAC1P_1.0',\n",
    "        \"ENG\" : 'ENG_1.0',\n",
    "        }\n",
    "    drop_cols = []\n",
    "    for i in cols:\n",
    "        if cols[i] == '':\n",
    "            drop_first = f'{i}_{df_processed[i].value_counts().idxmax()}'\n",
    "            cols[i] = drop_first\n",
    "        else:\n",
    "            drop_first = cols[i]\n",
    "        drop_cols.append(drop_first)\n",
    "        df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=True, columns=[i]) ; df_processed.drop(columns=drop_first, inplace=True)\n",
    "    return df_processed, df, target, sex, cols\n",
    "\n",
    "data, data_original, target, group, cols = data_processing(acs_data, feature_names, target_name)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0)\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf, group_train_rf, group_test_rf = train_test_split(\n",
    "    data_original, target, group, test_size=0.2, random_state=0)\n",
    "X_test_rf.drop(columns=['target', 'PINCP'], inplace=True)\n",
    "X_train_rf.drop(columns=['target', 'PINCP'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The details of our \"example\" person\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Person Explanation\n",
    "When encoding the discrete variables of our data. We chose the reference datapoint from Table: XX.\n",
    "We chose to use this reference person as we assert this person is the \"generic\" american citizen in this case. <br>\n",
    "| **Variable** | **Value** | **Reason** |\n",
    "| --------- | -------- | --------- |\n",
    "| HINS1 | 2 | As a rule, we assume that respondants don't have any form of insurance. |\n",
    "| HINS2 | 2 | Same as above. |\n",
    "| HINS4 | 2 | Same as above. | \n",
    "| CIT | 1 | We assume that respondants would be american-born. |\n",
    "| COW | 1 | We find that working for a private for-profit company is the most \"baseline\" occupation. |\n",
    "| SCHL | 16 | We set the reference as High school, since it fits nicely in the middle of the variable. | \n",
    "| MAR | 5 | Non-married as reference. Able to compare with the starting point in ones love life. |\n",
    "| SEX | 1 | Male as reference. Slight majority in data. |\n",
    "| ENG | 1 | Use very well english speaking as reference, since the country is english speaking natively.|\n",
    "\n",
    "\n",
    "The overall though process for choosing the reference variables was to choose the variables which best capture the picture of the \"ideal average\" citizen in the dataset. <br>\n",
    "We use min-max scaling on the \"AGEP\" variable, to ensure that our logistic regression model converges within a reasonable time-frame. <br>\n",
    "In terms of model parameters we copied the Logistic regression parameters from exercise 2 and found that they work quite well for this project. The Random Forest is in initialized with 100 estimators and a max depth of 10. With these parameters we strike a balance between a lightweight tree model and still having the model be complex enough that it stays black-box. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline, X_train, y_train, X_test, y_test, threshold1, threshold2, rf):\n",
    "    global s1, s2, preds_s1, preds_s2, true_s1, true_s2, fpr1, tpr1, thresholds1, fpr2, tpr2, thresholds2, scores_s1, scores_s2   \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    if not rf:\n",
    "        s2 = X_test.loc[X_test['SEX_2.0'] == True] \n",
    "        s1 = X_test.loc[X_test['SEX_2.0'] == False]\n",
    "    if rf:\n",
    "        s1 = X_test.loc[X_test['SEX'] == 1]\n",
    "        s2 = X_test.loc[X_test['SEX'] == 2]\n",
    "\n",
    "    true_s1 = []\n",
    "    for i in s1.index:\n",
    "        true_s1.append(y_test[i])\n",
    "    true_s2 = []\n",
    "    for i in s2.index:\n",
    "        true_s2.append(y_test[i])\n",
    "\n",
    "    if not rf:\n",
    "        scores_s1 = pipeline.predict_proba(s1)[:, 1]\n",
    "        fpr1, tpr1, thresholds1 = roc_curve(true_s1, scores_s1, pos_label=1)\n",
    "        scores_s2 = pipeline.predict_proba(s2)[:, 1]\n",
    "        fpr2, tpr2, thresholds2 = roc_curve(true_s2, scores_s2, pos_label=1)\n",
    "    if rf:\n",
    "        scores_s1 = pipeline.predict_proba(s1)[:, 1]\n",
    "        fpr1, tpr1, thresholds1 = roc_curve(true_s1, scores_s1)\n",
    "        scores_s2 = pipeline.predict_proba(s2)[:, 1]\n",
    "        fpr2, tpr2, thresholds2 = roc_curve(true_s2, scores_s2)\n",
    "\n",
    "    preds_s1 = scores_s1 > threshold1\n",
    "    preds_s2 = scores_s2 > threshold2\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_odd_diff(true_s1, true_s2, scores_1, scores_2, k):\n",
    "    x = np.linspace(0, 1, k)\n",
    "    fpr_diff = []\n",
    "    tpr_diff = []\n",
    "    accuracies_1 = []\n",
    "    accuracies_2 = []\n",
    "    best_vals = {'threshold' : None, 'diff' : 1}\n",
    "    counter = 0\n",
    "    for idx, i1 in enumerate(x):\n",
    "        for idxx, i2 in enumerate(x):    \n",
    "            preds_s1 = scores_1 >= i1\n",
    "            preds_s2 = scores_2 >= i2\n",
    "            tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "            tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "            fpr1 = fp1 / (fp1 + tn1)\n",
    "            tpr1 = tp1 / (tp1 + fn1)\n",
    "            fpr2 = fp2 / (fp2 + tn2)\n",
    "            tpr2 = tp2 / (tp2 + fn2)\n",
    "            fpr_diff.append(abs(fpr1 - fpr2))\n",
    "            tpr_diff.append(abs(tpr1 - tpr2))\n",
    "            accuracies_1.append(accuracy_score(preds_s1, true_s1))\n",
    "            accuracies_2.append(accuracy_score(preds_s2, true_s2))\n",
    "            if abs(tpr_diff[counter] - fpr_diff[counter]) < abs(best_vals['diff']):\n",
    "                best_vals['diff'] = abs(tpr_diff[counter] - fpr_diff[counter])\n",
    "                best_vals['threshold'] = [i1, i2]\n",
    "            counter += 1\n",
    "    return best_vals, tpr_diff, fpr_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to make sure logisric regression converges\n",
    "scaler = MinMaxScaler()\n",
    "X_train['AGEP'] = scaler.fit_transform(X_train[['AGEP']])\n",
    "X_test['AGEP'] = scaler.transform(X_test[['AGEP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_args = {\n",
    "    'n_estimators': 100, # changed from 100\n",
    "    'max_depth' : 10, #changed from 10\n",
    "    'min_samples_split': 2, #changed from 2\n",
    "    'min_samples_leaf': 2, #changed from 2\n",
    "    'random_state': 0\n",
    "}\n",
    "lr_model = LogisticRegression(max_iter=5000, penalty= \"l2\", C= 0.8497534359086438, tol=1e-4, solver = \"saga\", random_state=0)\n",
    "p1 = RandomForestClassifier(**rf_args)\n",
    "p2 = lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_parity(preds_s1, preds_s2, s1, s2):\n",
    "    parity = (np.sum(preds_s2)/len(s2)) / (np.sum(preds_s1)/len(s1))\n",
    "    if parity > 1:\n",
    "        parity = (np.sum(preds_s1)/len(s1)) / (np.sum(preds_s2)/len(s2))\n",
    "        print('Group 2 is underrepresented by', parity)\n",
    "    else:\n",
    "        print('Group 1 is underrepresented by', parity)\n",
    "\n",
    "def equalized_odds(tpr1, fpr1, tpr2, fpr2):\n",
    "    fpr_diff = fpr1 / fpr2\n",
    "    if fpr_diff > 1:\n",
    "        fpr_diff = fpr2 / fpr1\n",
    "        print('Group 2 is underrepresented by', fpr_diff, '(FPR)')\n",
    "    else:\n",
    "        fpr_diff = fpr1 / fpr2\n",
    "        print('Group 1 is underrepresented by', fpr_diff, '(FPR)')\n",
    "    tpr_diff = tpr1 / tpr2\n",
    "    if tpr_diff > 1:\n",
    "        tpr_diff = tpr2 / tpr1\n",
    "        print('Group 2 is underrepresented by', tpr_diff, '(TPR)')\n",
    "    else:\n",
    "        tpr_diff = tpr1 / tpr2\n",
    "        print('Group 1 is underrepresented by', tpr_diff, '(TPR)')\n",
    "\n",
    "def equalized_outcomes(some_1, some_2, some_3, some4):\n",
    "    p_diff = some_1 / some_2\n",
    "    if p_diff > 1:\n",
    "        p_diff = some_2 / some_1\n",
    "        print('Group 2 is underrepresented by', p_diff, '(positives)')\n",
    "    else:\n",
    "        p_diff = some_1 / some_2\n",
    "        print('Group 1 is underrepresented by', p_diff, '(positives)')\n",
    "    n_diff = some_3 / some4\n",
    "    if n_diff > 1:\n",
    "        n_diff = some4 / some_3\n",
    "        print('Group 2 is underrepresented by', n_diff, '(negatives)')\n",
    "    else:\n",
    "        n_diff = some_3 / some4\n",
    "        print('Group 1 is underrepresented by', n_diff, '(negatives)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_done = evaluate_pipeline(p2, X_train, y_train, X_test, y_test, 0.5, 0.5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr1, tpr1, label='Group 1')\n",
    "plt.plot(fpr2, tpr2, label='Group 2')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('------------BEFORE BIAS ANALYSIS------------')\n",
    "\n",
    "stat_parity(preds_s1, preds_s2, s1, s2)\n",
    "\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "\n",
    "best_vals, balls1, balls2 = minimize_odd_diff(true_s1=true_s1, true_s2=true_s2, scores_1= scores_s1,scores_2= scores_s2, k= 50)\n",
    "\n",
    "preds_s1 = scores_s1 >= best_vals['threshold'][0]\n",
    "preds_s2 = scores_s2 >= best_vals['threshold'][1]\n",
    "\n",
    "# equalized odds (horizontal)\n",
    "tpr1 = tp1 / (tp1 + fn1)\n",
    "fpr1 = fp1 / (fp1 + tn1)\n",
    "tpr2 = tp2 / (tp2 + fn2)\n",
    "fpr2 = fp2 / (fp2 + tn2)\n",
    "\n",
    "equalized_odds(tpr1, fpr1, tpr2, fpr2)\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "equalized_outcomes(some_1, some_2, some_3, some_4)\n",
    "\n",
    "print('------------AFTER BIAS ANALYSIS------------')\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "\n",
    "stat_parity(preds_s1, preds_s2, s1, s2)\n",
    "\n",
    "fpr_group1 = fp1 / (fp1 + tn1)\n",
    "tpr_group1 = tp1 / (tp1 + fn1)\n",
    "fpr_group2 = fp2 / (fp2 + tn2)\n",
    "tpr_group2 = tp2 / (tp2 + fn2)\n",
    "\n",
    "equalized_odds(tpr_group1, fpr_group1, tpr_group2, fpr_group2)\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "equalized_outcomes(some_1, some_2, some_3, some_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf['AGEP'] = scaler.transform(X_train_rf[['AGEP']])\n",
    "X_test_rf['AGEP'] = scaler.transform(X_test_rf[['AGEP']])\n",
    "p1_done = evaluate_pipeline(p1, X_train_rf, y_train_rf, X_test_rf, y_test_rf, 0.5, 0.5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr1, tpr1, label='Group 1')\n",
    "plt.plot(fpr2, tpr2, label='Group 2')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('------------BEFORE BIAS ANALYSIS------------')\n",
    "\n",
    "stat_parity(preds_s1, preds_s2, s1, s2)\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "\n",
    "best_vals, balls1, balls2 = minimize_odd_diff(true_s1=true_s1, true_s2=true_s2, scores_1= scores_s1,scores_2= scores_s2, k= 50)\n",
    "\n",
    "preds_s1 = scores_s1 >= best_vals['threshold'][0]\n",
    "preds_s2 = scores_s2 >= best_vals['threshold'][1]\n",
    "\n",
    "# equalized odds (horizontal)\n",
    "tpr1 = tp1 / (tp1 + fn1)\n",
    "fpr1 = fp1 / (fp1 + tn1)\n",
    "tpr2 = tp2 / (tp2 + fn2)\n",
    "fpr2 = fp2 / (fp2 + tn2)\n",
    "\n",
    "equalized_odds(tpr1, fpr1, tpr2, fpr2)\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "equalized_outcomes(some_1, some_2, some_3, some_4)\n",
    "\n",
    "print('------------AFTER BIAS ANALYSIS------------')\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "\n",
    "stat_parity(preds_s1, preds_s2, s1, s2)\n",
    "\n",
    "fpr_group1 = fp1 / (fp1 + tn1)\n",
    "tpr_group1 = tp1 / (tp1 + fn1)\n",
    "fpr_group2 = fp2 / (fp2 + tn2)\n",
    "tpr_group2 = tp2 / (tp2 + fn2)\n",
    "\n",
    "equalized_odds(tpr_group1, fpr_group1, tpr_group2, fpr_group2)\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "equalized_outcomes(some_1, some_2, some_3, some_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (Explaining white-box models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 (Which features for the Logistic Regression are most relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying scaling to the relevant columns\n",
    "scaler = MinMaxScaler()\n",
    "X_train[[\"AGEP\"]] = scaler.fit_transform(X_train[[\"AGEP\"]])\n",
    "X_test[[\"AGEP\"]] = scaler.fit_transform(X_test[[\"AGEP\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the logistic regression model\n",
    "model = LogisticRegression(max_iter=5000, penalty= \"l2\", C= 0.8497534359086438, tol=1e-4, solver = \"saga\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the feature importance for the logistic regression model\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 and bottom 5 coefficients\n",
    "top_5_coeffs = coefficients_df.head(5)\n",
    "bottom_5_coeffs = coefficients_df.tail(5)\n",
    "\n",
    "# Concatenate the top and bottom coefficients\n",
    "coefficients = pd.concat([top_5_coeffs, bottom_5_coeffs])\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(coefficients['Feature'], coefficients['Coefficient'])\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Five highest and lowest Coefficients')\n",
    "\n",
    "# Add values to the bars\n",
    "for i, coeff in enumerate(coefficients['Coefficient']):\n",
    "    plt.text(i, coeff, str(round(coeff, 2)), ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy of the model\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 (Counterfactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the predicted probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "probs_df = pd.DataFrame({'Index': X_test.index, 'Probability 0': probs[:,0], \"probability 1\": probs[:,1], \"predicted\": model.predict(X_test), 'Actual': y_test, \"Sex\" : X_test[\"SEX_2.0\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute difference between probability 0 and probability 1\n",
    "probs_df['diff'] = abs(probs_df['Probability 0'] - probs_df['probability 1'])\n",
    "# Sort the dataframe by the absolute difference\n",
    "probs_df_sorted = probs_df.sort_values('diff')\n",
    "# Select the top 5 rows with the closest probabilities\n",
    "selected_people = probs_df_sorted.head(3)\n",
    "# Print the selected people\n",
    "print(selected_people)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose one person for the counterfactual from the list of people with the closest probabilities \n",
    "person = X_test[24663:24664]\n",
    "#Change original value of sex to the opposite\n",
    "person.loc[person[\"SEX_2.0\"]] = False\n",
    "#make the new prediction\n",
    "new_pred = model.predict(person)\n",
    "# printing the counterfactual\n",
    "print(\"New prediction:\", new_pred[0], \"- Original prediction:\", probs_df['predicted'][24663], \"- New probability\", model.predict_proba(person))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (Model agnostic explainations with SHAP)\n",
    "\n",
    "Here we use model agnostic explainations with SHAP to explain both of the models. Summary of this section below:\n",
    "\n",
    " - Row chosen for comparison and printed\n",
    " - For Logistic Regression and then Random Forest:\n",
    "    1. Calculate SHAP values\n",
    "    2. Aggregate SHAP values according to one-hot encodings per feature\n",
    "    3. Generate bar plot\n",
    "    4. Generate beeswarm plot\n",
    "    5. Generate force plot for specific row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row chosen for comparison\n",
    "\n",
    "Row chosen for comparison is 0th index of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_row_idx = 0\n",
    "X_test.iloc[[specific_row_idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person is:\n",
    "\n",
    "- 62 years of age (not shown in code above as it is scaled, but gotten manually)\n",
    "- Male\n",
    "- Has some College, but less than 1 year\n",
    "- Has insurance through a current or former employee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating shap values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_explainer = shap.Explainer(p2_done, X_train, feature_names=X_train.columns)\n",
    "lr_shap_values = lr_explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating one-hot encodings to be per-feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on what features were onehot encoded get indices of rows that need to be aggregated\n",
    "group_idx_list = list()\n",
    "for i in feature_names:\n",
    "    group_idx_list.append([n for n, l in enumerate(X_train.columns) if l.startswith(i)])\n",
    "\n",
    "# for each row aggregate the shap values based on indices from group_idx_list and store them in shap_agg_full\n",
    "shap_agg_full = np.zeros((lr_shap_values.shape[0], len(feature_names)))\n",
    "for group_idx, group in enumerate(group_idx_list):\n",
    "    shap_agg_full[:, group_idx] = np.sum(lr_shap_values[:, group].values, axis=1)\n",
    "\n",
    "# to still have an explainer object we can copy and replace lr_shap_values\n",
    "# having an explainer object is needed for bar & beeswarm plots\n",
    "lr_shap_values_aggregated = lr_shap_values\n",
    "lr_shap_values_aggregated.values = shap_agg_full\n",
    "lr_shap_values_aggregated.data = lr_shap_values_aggregated.data[:, :len(feature_names)]\n",
    "lr_shap_values_aggregated.values.shape\n",
    "lr_shap_values_aggregated.feature_names = feature_names # shap_agg_full column order is the same as feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating general bar and beeswarm plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(lr_shap_values_aggregated, max_display=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(lr_shap_values_aggregated[:, 0:1], max_display=11) # indexxed for only AGEP feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating row specific plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(lr_explainer.expected_value, shap_agg_full[specific_row_idx], feature_names=feature_names, matplotlib=True, link='logit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating shap values - can take up to 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_explainer = shap.Explainer(p1_done, X_train_rf, model_output='probability', feature_names=X_train_rf.columns)\n",
    "rf_shap_values = rf_explainer(X_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating general bar and beeswarm plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(rf_shap_values[:,:,1], max_display=11)\n",
    "shap.plots.beeswarm(rf_shap_values[:,0:1,1], max_display=11) # indexxed for only AGEP feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating row specific plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_explain = X_test_rf.iloc[[specific_row_idx]]\n",
    "row_shap_values = rf_explainer(row_to_explain)\n",
    "shap.plots.force(rf_explainer.expected_value[1], row_shap_values.values[:, :, 1], row_to_explain, feature_names=X_train_rf.columns, matplotlib=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorithmic_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
