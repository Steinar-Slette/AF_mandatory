{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "feature_names = ['AGEP', # Age\n",
    "                 \"CIT\", # Citizenship status\n",
    "                 'COW', # Class of worker\n",
    "                 \"ENG\", # Ability to speak English\n",
    "                 'SCHL', # Educational attainment\n",
    "                 'MAR', # Marital status\n",
    "                 \"HINS1\", # Insurance through a current or former employer or union\n",
    "                 \"HINS2\", # Insurance purchased directly from an insurance company\n",
    "                 \"HINS4\", # Medicaid\n",
    "                 \"RAC1P\", # Recoded detailed race code\n",
    "                 'SEX']\n",
    "\n",
    "target_name = \"PINCP\" # Total person's income\n",
    "\n",
    "def data_processing(data, features, target_name:str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[features + [\"target\", target_name]] ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "    cols = {\n",
    "        \"HINS1\": 'HINS1_2.0',\n",
    "        \"HINS2\": 'HINS2_2.0',\n",
    "        \"HINS4\": 'HINS4_2.0',\n",
    "        \"CIT\": 'CIT_1.0',\n",
    "        \"COW\" : 'COW_1.0',\n",
    "        \"SCHL\" : 'SCHL_16.0',\n",
    "        \"MAR\": 'MAR_5.0',\n",
    "        \"SEX\": 'SEX_1.0',\n",
    "        \"RAC1P\" :'RAC1P_1.0',\n",
    "        \"ENG\" : 'ENG_1.0',\n",
    "        }\n",
    "    drop_cols = []\n",
    "    for i in cols:\n",
    "        if cols[i] == '':\n",
    "            drop_first = f'{i}_{df_processed[i].value_counts().idxmax()}'\n",
    "            cols[i] = drop_first\n",
    "        else:\n",
    "            drop_first = cols[i]\n",
    "        drop_cols.append(drop_first)\n",
    "        df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=True, columns=[i]) ; df_processed.drop(columns=drop_first, inplace=True)\n",
    "    return df_processed, df, target, sex, cols\n",
    "\n",
    "data, data_original, target, group, cols = data_processing(acs_data, feature_names, target_name)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols\n",
    "# The details of our \"example\" person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Person Explanation\n",
    "When encoding the discrete variables of our data. We chose the reference datapoint from Table: XX.\n",
    "We chose to use this reference person as we assert this person is the \"generic\" american citizen in this case. <br>\n",
    "| **Variable** | **Value** | **Reason** |\n",
    "| --------- | -------- | --------- |\n",
    "| HINS1 | 2 | As a rule, we assume that respondants don't have any form of insurance. |\n",
    "| HINS2 | 2 | Same as above. |\n",
    "| HINS4 | 2 | Same as above. | \n",
    "| CIT | 1 | We assume that respondants would be american-born. |\n",
    "| COW | 1 | We find that working for a private for-profit company is the most \"baseline\" occupation. |\n",
    "| SCHL | 16 | We set the reference as High school, since it fits nicely in the middle of the variable. | \n",
    "| MAR | 5 | Non-married as reference. Able to compare with the starting point in ones love life. |\n",
    "| SEX | 1 | Male as reference. Slight majority in data. |\n",
    "| ENG | 1 | Use very well english speaking as reference, since the country is english speaking natively.|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest to Classify if target is above income threshold\n",
    "\n",
    "# Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline, X_train, y_train, X_test, y_test, threshold1, threshold2):\n",
    "    global s1, s2, preds_s1, preds_s2, true_s1, true_s2, fpr1, tpr1, thresholds1, fpr2, tpr2, thresholds2, scores_s1, scores_s2   \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # y_pred = pipeline.predict(X_test)\n",
    "    # Modify pipeline to fulfill one of the fairness criteria (e.g. statistical parity, equal opportunity, etc.)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    s2 = X_test.loc[X_test['SEX_2.0'] == True] \n",
    "    s1 = X_test.loc[X_test['SEX_2.0'] == False]\n",
    "\n",
    "    true_s1 = []\n",
    "    for i in s1.index:\n",
    "        true_s1.append(y_test[i])\n",
    "    true_s2 = []\n",
    "    for i in s2.index:\n",
    "        true_s2.append(y_test[i])\n",
    "\n",
    "\n",
    "    scores_s1 = pipeline.predict_proba(s1)[:, 1]\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(true_s1, scores_s1, pos_label=1)\n",
    "    scores_s2 = pipeline.predict_proba(s2)[:, 1]\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(true_s2, scores_s2, pos_label=1)\n",
    "\n",
    "\n",
    "    preds_s1 = scores_s1 > threshold1\n",
    "    preds_s2 = scores_s2 > threshold2\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to make sure logisric regression converges\n",
    "X_train['AGEP'] = MinMaxScaler().fit_transform(X_train[['AGEP']])\n",
    "X_test['AGEP'] = MinMaxScaler().fit_transform(X_test[['AGEP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_args = {\n",
    "    'n_estimators': 100, # changed from 100\n",
    "    'max_depth' : 10, #changed from 10\n",
    "    'min_samples_split': 2, #changed from 2\n",
    "    'min_samples_leaf': 2, #changed from 2\n",
    "    'random_state': 0\n",
    "}\n",
    "lr_model = LogisticRegression(max_iter=5000, penalty= \"l2\", C= 0.8497534359086438, tol=1e-4, solver = \"saga\", random_state=0)\n",
    "p1 = RandomForestClassifier(**rf_args)\n",
    "p2 = lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_done = evaluate_pipeline(p2, X_train, y_train, X_test, y_test, 0.4, 0.5)\n",
    "\n",
    "plt.plot(fpr1, tpr1, label='Group 1')\n",
    "plt.plot(fpr2, tpr2, label='Group 2')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "# Calculate Youden's J statistic for each model\n",
    "youden_j1 = tpr1 - fpr1\n",
    "optimal_threshold_index1 = np.argmax(youden_j1)\n",
    "optimal_threshold1 = thresholds1[optimal_threshold_index1]\n",
    "\n",
    "youden_j2 = tpr2 - fpr2\n",
    "optimal_threshold_index2 = np.argmax(youden_j2)\n",
    "optimal_threshold2 = thresholds2[optimal_threshold_index2]\n",
    "preds_s1 = scores_s1 > 0.66 # 0.66 best \n",
    "preds_s2 = scores_s2 > optimal_threshold2 # 0.4756088801681863 best\n",
    "print(\"Optimal threshold for group 1: \", optimal_threshold1)\n",
    "print(\"Optimal threshold for group 2: \", optimal_threshold2)\n",
    "\n",
    "print(\"Accuracy for group 1: \", accuracy_score(true_s1, preds_s1))\n",
    "print(\"Accuracy for group 2: \", accuracy_score(true_s2, preds_s2))\n",
    "\n",
    "print('Selection rate for s1: ', np.sum(preds_s1)/len(s1))\n",
    "print('Selection rate for s2: ', np.sum(preds_s2)/len(s2))\n",
    "\n",
    "statistical_parity = np.sum(preds_s1)/len(s1) - np.sum(preds_s2)/len(s2)\n",
    "print('Statistical parity: ', abs(statistical_parity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_done = evaluate_pipeline(p1, X_train, y_train, X_test, y_test, 0.4, 0.5)\n",
    "\n",
    "plt.plot(fpr1, tpr1, label='Group 1')\n",
    "plt.plot(fpr2, tpr2, label='Group 2')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Youden's J statistic for each model\n",
    "youden_j1 = tpr1 - fpr1\n",
    "optimal_threshold_index1 = np.argmax(youden_j1)\n",
    "optimal_threshold1 = thresholds1[optimal_threshold_index1]\n",
    "\n",
    "youden_j2 = tpr2 - fpr2\n",
    "optimal_threshold_index2 = np.argmax(youden_j2)\n",
    "optimal_threshold2 = thresholds2[optimal_threshold_index2]\n",
    "preds_s1 = scores_s1 > 0.66 # 0.66 best \n",
    "preds_s2 = scores_s2 > optimal_threshold2 # 0.4756088801681863 best\n",
    "print(\"Optimal threshold for group 1: \", optimal_threshold1)\n",
    "print(\"Optimal threshold for group 2: \", optimal_threshold2)\n",
    "\n",
    "p1_done = evaluate_pipeline(p1, X_train, y_train, X_test, y_test, 0.401, optimal_threshold2)\n",
    "print(\"Accuracy for group 1: \", accuracy_score(true_s1, preds_s1))\n",
    "print(\"Accuracy for group 2: \", accuracy_score(true_s2, preds_s2))\n",
    "\n",
    "print('Selection rate for s1: ', np.sum(preds_s1)/len(s1))\n",
    "print('Selection rate for s2: ', np.sum(preds_s2)/len(s2))\n",
    "\n",
    "statistical_parity = np.sum(preds_s1)/len(s1) - np.sum(preds_s2)/len(s2)\n",
    "print('Statistical parity: ', abs(statistical_parity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes to be made to models\n",
    "The accuracy of the classifier sits at 0.77 as of baseline test with the selected pipeline. Standard-scaling was used across all features, which may end up causing trouble. The model has a fairly high accuracy for a baseline classifier, with desirable f1-scores at [0.72, 0.81]. Since the outcome variable in this case is almost equally balanced at the chosen threshold, we need not to scale our model much with respect to the outcome variable. In this pipeline discrete variables are scaled, which changes the otherwise explainable binary variables, which hurts the explainability of the model. <br>\n",
    "<br>\n",
    "We would need to only scale the age column in the data to fix this explainability issue. Unless we had more continuous ways of representing the discrete variables in the data (such as language test scores for english proficiency, SAT scores for education etc.) we would not be able to quantify these columns in any meaningful fashion.\n",
    "\n",
    "### What we did\n",
    "In trying to improve the interpretability of our model, we limit the scaling to exclusively be applied to the \"AGEP\" column, seeing as it is the only continuous variable in the data. We also use the template's onehot-encodings applied to the discrete variables, with dropping the first value applied. This way we can compare each categorical variable to the baseline of the one which is dropped. <br>\n",
    "\n",
    "Before making any changes to any of the models we find the following accuracies of the classifiers: \n",
    "<br>\n",
    "(insert classification report for models)\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model agnostic explainations with SHAP\n",
    "\n",
    "Here we use model agnostic explainations with SHAP to explain both of the models.\n",
    "\n",
    "We first use a plot that takes all data rows to explain a bit generally the model overall.\n",
    "\n",
    "Then we use take some specific rows of data in an attempt to explain more specific aspects of the models.\n",
    "\n",
    "For features that are one-hot encoded we aggregate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import shap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows chosen for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_rows = [0, 1, 2, 3, 4] # for now just all rows used for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating SHAP values...')\n",
    "X_train_ = X_train.to_numpy().astype(float)\n",
    "X_test_ = X_test.to_numpy().astype(float)\n",
    "lr_explainer = shap.Explainer(\n",
    "    p2,\n",
    "    X_train_,\n",
    "    # model_output='probability',\n",
    "    # link=shap.links.logit,\n",
    "    # algorithm='linear',\n",
    "    feature_names=X_train.columns\n",
    "    )\n",
    "\n",
    "lr_shap_values = lr_explainer(X_test_)\n",
    "\n",
    "print('Generating general bar plot...')\n",
    "shap.plots.bar(lr_shap_values, max_display=8)\n",
    "\n",
    "print('Generating general beeswarm plot...')\n",
    "shap.plots.beeswarm(lr_shap_values, max_display=8, show=False)\n",
    "ax = plt.gca()\n",
    "# You can change the min and max value of xaxis by changing the arguments of:\n",
    "ax.set_xlim(-3.5, 3.5) \n",
    "plt.show()\n",
    "\n",
    "print('Generating row specific plots... - UNUSED')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    # shap.force_plot(shap_values[i], link='logit', matplotlib=True)\n",
    "    shap.force_plot(lr_shap_values[i], matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_idx = list()\n",
    "shap_agg_full = np.zeros((lr_shap_values.shape[0], len(X_train.columns)))\n",
    "for i in feature_names:\n",
    "    group_idx.append([n for n, l in enumerate(X_train.columns) if l.startswith(i)])\n",
    "\n",
    "# group_idx now contains the indices of the features that belong to each group\n",
    "# we can now sum the SHAP values for each group\n",
    "    \n",
    "for ii, g in enumerate(group_idx):\n",
    "    # print(np.sum(lr_shap_values[:, g].values, -1))\n",
    "    # print(shap_agg_full[:, ii])\n",
    "    shap_agg_full[:, ii] = np.sum(lr_shap_values[:, g].values, -1)\n",
    "\n",
    "# copy and replace? - YES\n",
    "lr_shap_values_copy = lr_shap_values\n",
    "lr_shap_values_copy.values = shap_agg_full\n",
    "\n",
    "# now can do the force plots again\n",
    "print('Generating row specific plots with aggregated categories...')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    # shap.force_plot(shap_values[i], link='logit', matplotlib=True)\n",
    "    shap.force_plot(lr_shap_values_copy[i], feature_names=feature_names, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating SHAP values... can take up to 15 mins...')\n",
    "X_train_ = X_train.to_numpy().astype(float)\n",
    "X_test_ = X_test.to_numpy().astype(float)\n",
    "rf_explainer = shap.Explainer(p1, X_train_, model_output='probability', feature_names=X_train.columns)\n",
    "rf_shap_values = rf_explainer(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating general bar plot...')\n",
    "shap.plots.bar(rf_shap_values[:,:,1], max_display=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating general beeswarm plot...')\n",
    "shap.plots.beeswarm(rf_shap_values[:,:,1], max_display=8)\n",
    "\n",
    "# unused scaling:\n",
    "# ax = plt.gca()\n",
    "# # You can change the min and max value of xaxis by changing the arguments of:\n",
    "# ax.set_xlim(-3.5, 3.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating row specific plots...')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    row_to_explain = X_test_[i]\n",
    "    row_shap_values = rf_explainer(row_to_explain)\n",
    "    shap.plots.force(rf_explainer.expected_value[1], row_shap_values.values[:,1], row_to_explain, feature_names=X_train.columns, matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now can do the force plots again\n",
    "print('Generating row specific plots with aggregated categories...')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    \n",
    "    row_to_explain = X_test_[i]\n",
    "    row_shap_values = rf_explainer(row_to_explain)\n",
    "    \n",
    "    group_idx = list()\n",
    "    shap_agg_row = np.zeros((row_shap_values.shape[0], 2))\n",
    "    # print(shap_agg_row.shape)\n",
    "    for i in feature_names:\n",
    "        group_idx.append([n for n, l in enumerate(X_train.columns) if l.startswith(i)])\n",
    "\n",
    "    for idx, g in enumerate(group_idx):\n",
    "        shap_agg_row[idx, 1] = np.sum(row_shap_values[g, 1].values, -1)\n",
    "\n",
    "    row_shap_values_copy = row_shap_values\n",
    "    row_shap_values_copy.values = shap_agg_row\n",
    "\n",
    "    shap.plots.force(rf_explainer.expected_value[1], row_shap_values.values[:,1], row_to_explain, feature_names=feature_names, matplotlib=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
