{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "feature_names = ['AGEP', # Age\n",
    "                 \"CIT\", # Citizenship status\n",
    "                 'COW', # Class of worker\n",
    "                 \"ENG\", # Ability to speak English\n",
    "                 'SCHL', # Educational attainment\n",
    "                 'MAR', # Marital status\n",
    "                 \"HINS1\", # Insurance through a current or former employer or union\n",
    "                 \"HINS2\", # Insurance purchased directly from an insurance company\n",
    "                 \"HINS4\", # Medicaid\n",
    "                 \"RAC1P\", # Recoded detailed race code\n",
    "                 'SEX']\n",
    "\n",
    "target_name = \"PINCP\" # Total person's income\n",
    "\n",
    "def data_processing(data, features, target_name:str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[features + [\"target\", target_name]] ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "    cols = {\n",
    "        \"HINS1\": 'HINS1_2.0',\n",
    "        \"HINS2\": 'HINS2_2.0',\n",
    "        \"HINS4\": 'HINS4_2.0',\n",
    "        \"CIT\": 'CIT_1.0',\n",
    "        \"COW\" : 'COW_1.0',\n",
    "        \"SCHL\" : 'SCHL_16.0',\n",
    "        \"MAR\": 'MAR_5.0',\n",
    "        \"SEX\": 'SEX_1.0',\n",
    "        \"RAC1P\" :'RAC1P_1.0',\n",
    "        \"ENG\" : 'ENG_1.0',\n",
    "        }\n",
    "    drop_cols = []\n",
    "    for i in cols:\n",
    "        if cols[i] == '':\n",
    "            drop_first = f'{i}_{df_processed[i].value_counts().idxmax()}'\n",
    "            cols[i] = drop_first\n",
    "        else:\n",
    "            drop_first = cols[i]\n",
    "        drop_cols.append(drop_first)\n",
    "        df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=True, columns=[i]) ; df_processed.drop(columns=drop_first, inplace=True)\n",
    "    return df_processed, df, target, sex, cols\n",
    "\n",
    "data, data_original, target, group, cols = data_processing(acs_data, feature_names, target_name)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols\n",
    "# The details of our \"example\" person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Person Explanation\n",
    "When encoding the discrete variables of our data. We chose the reference datapoint from Table: XX.\n",
    "We chose to use this reference person as we assert this person is the \"generic\" american citizen in this case. <br>\n",
    "| **Variable** | **Value** | **Reason** |\n",
    "| --------- | -------- | --------- |\n",
    "| HINS1 | 2 | As a rule, we assume that respondants don't have any form of insurance. |\n",
    "| HINS2 | 2 | Same as above. |\n",
    "| HINS4 | 2 | Same as above. | \n",
    "| CIT | 1 | We assume that respondants would be american-born. |\n",
    "| COW | 1 | We find that working for a private for-profit company is the most \"baseline\" occupation. |\n",
    "| SCHL | 16 | We set the reference as High school, since it fits nicely in the middle of the variable. | \n",
    "| MAR | 5 | Non-married as reference. Able to compare with the starting point in ones love life. |\n",
    "| SEX | 1 | Male as reference. Slight majority in data. |\n",
    "| ENG | 1 | Use very well english speaking as reference, since the country is english speaking natively.|\n",
    "\n",
    "\n",
    "The overall though process for choosing the reference variables was to choose the variables which best capture the picture of the \"ideal average\" citizen in the dataset. <br>\n",
    "We use min-max scaling on the \"AGEP\" variable, to ensure that our logistic regression model converges within a reasonable time-frame. <br>\n",
    "In terms of model parameters we copied the Logistic regression parameters from exercise 2 and found that they work quite well for this project. The Random Forest is in initialized with 100 estimators and a max depth of 10. With these parameters we strike a balance between a lightweight tree model and still having the model be complex enough that it stays black-box. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forest to Classify if target is above income threshold\n",
    "\n",
    "# Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline, X_train, y_train, X_test, y_test, threshold1, threshold2):\n",
    "    global s1, s2, preds_s1, preds_s2, true_s1, true_s2, fpr1, tpr1, thresholds1, fpr2, tpr2, thresholds2, scores_s1, scores_s2   \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # y_pred = pipeline.predict(X_test)\n",
    "    # Modify pipeline to fulfill one of the fairness criteria (e.g. statistical parity, equal opportunity, etc.)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    s2 = X_test.loc[X_test['SEX_2.0'] == True] \n",
    "    s1 = X_test.loc[X_test['SEX_2.0'] == False]\n",
    "\n",
    "    true_s1 = []\n",
    "    for i in s1.index:\n",
    "        true_s1.append(y_test[i])\n",
    "    true_s2 = []\n",
    "    for i in s2.index:\n",
    "        true_s2.append(y_test[i])\n",
    "\n",
    "\n",
    "    scores_s1 = pipeline.predict_proba(s1)[:, 1]\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(true_s1, scores_s1, pos_label=1)\n",
    "    scores_s2 = pipeline.predict_proba(s2)[:, 1]\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(true_s2, scores_s2, pos_label=1)\n",
    "\n",
    "\n",
    "    preds_s1 = scores_s1 > threshold1\n",
    "    preds_s2 = scores_s2 > threshold2\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_odd_diff(true_s1, true_s2, scores_1, scores_2, k):\n",
    "    x = np.linspace(0, 1, k)\n",
    "    fpr_diff = []\n",
    "    tpr_diff = []\n",
    "    accuracies_1 = []\n",
    "    accuracies_2 = []\n",
    "    best_vals = {'threshold' : 0, 'diff' : 1}\n",
    "    for idx, i in enumerate(x):\n",
    "        preds_s1 = scores_1 >= i\n",
    "        preds_s2 = scores_2 >= i\n",
    "        tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "        tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "        fpr1 = fp1 / (fp1 + tn1)\n",
    "        tpr1 = tp1 / (tp1 + fn1)\n",
    "        fpr2 = fp2 / (fp2 + tn2)\n",
    "        tpr2 = tp2 / (tp2 + fn2)\n",
    "        fpr_diff.append(abs(fpr1 - fpr2))\n",
    "        tpr_diff.append(abs(tpr1 - tpr2))\n",
    "        accuracies_1.append(accuracy_score(preds_s1, true_s1))\n",
    "        accuracies_2.append(accuracy_score(preds_s2, true_s2))\n",
    "        if abs(tpr_diff[idx] - fpr_diff[idx]) < best_vals['diff']:\n",
    "            best_vals['diff'] = abs(tpr_diff[idx] - fpr_diff[idx])\n",
    "            best_vals['threshold'] = i\n",
    "    return best_vals, tpr_diff, fpr_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to make sure logisric regression converges\n",
    "X_train['AGEP'] = MinMaxScaler().fit_transform(X_train[['AGEP']])\n",
    "X_test['AGEP'] = MinMaxScaler().fit_transform(X_test[['AGEP']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_args = {\n",
    "    'n_estimators': 100, # changed from 100\n",
    "    'max_depth' : 10, #changed from 10\n",
    "    'min_samples_split': 2, #changed from 2\n",
    "    'min_samples_leaf': 2, #changed from 2\n",
    "    'random_state': 0\n",
    "}\n",
    "lr_model = LogisticRegression(max_iter=5000, penalty= \"l2\", C= 0.8497534359086438, tol=1e-4, solver = \"saga\", random_state=0)\n",
    "p1 = RandomForestClassifier(**rf_args)\n",
    "p2 = lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_done = evaluate_pipeline(p2, X_train, y_train, X_test, y_test, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(fpr1, tpr1, label='Group 1')\n",
    "plt.plot(fpr2, tpr2, label='Group 2')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('------------BEFORE BIAS ANALYSIS------------')\n",
    "\n",
    "statistical_parity = np.sum(preds_s1)/len(s1) - np.sum(preds_s2)/len(s2)\n",
    "print('Statistical parity: ', (statistical_parity))\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "\n",
    "best_vals, balls1, balls2 = minimize_odd_diff(true_s1=true_s1, true_s2=true_s2, scores_1= scores_s1,scores_2= scores_s2, k= 1000)\n",
    "\n",
    "preds_s1 = scores_s1 >= best_vals['threshold']\n",
    "preds_s2 = scores_s2 >= best_vals['threshold']\n",
    "\n",
    "# equalized odds (horizontal)\n",
    "tpr1 = tp1 / (tp1 + fn1)\n",
    "fpr1 = fp1 / (fp1 + tn1)\n",
    "tpr2 = tp2 / (tp2 + fn2)\n",
    "fpr2 = fp2 / (fp2 + tn2)\n",
    "\n",
    "print('FPR diff: ', abs(fpr1 - fpr2))\n",
    "print('TPR Diff', abs(tpr1 - tpr2))\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "print('positive outcome:', abs(some_1 - some_2))\n",
    "print('negative outcome:', abs(some_3 - some_4))\n",
    "\n",
    "\n",
    "print('------------AFTER BIAS ANALYSIS------------')\n",
    "\n",
    "statistical_parity = np.sum(preds_s1)/len(s1) - np.sum(preds_s2)/len(s2)\n",
    "print('Statistical parity: ', (statistical_parity))\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "fpr_group1 = fp1 / (fp1 + tn1)\n",
    "tpr_group1 = tp1 / (tp1 + fn1)\n",
    "\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "fpr_group2 = fp2 / (fp2 + tn2)\n",
    "tpr_group2 = tp2 / (tp2 + fn2)\n",
    "\n",
    "# equalized odds (horizontal)\n",
    "print('FPR diff: ', abs(fpr_group1 - fpr_group2))\n",
    "print('TPR Diff', abs(tpr_group1 - tpr_group2))\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "print('positive outcome diff:', abs(some_1 - some_2))\n",
    "print('negative outcome diff:', abs(some_3 - some_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.plot(np.linspace(0, 1, 1000),balls1, label = 'tpr_diff')\n",
    "plt.plot(np.linspace(0, 1, 1000), balls2, label = 'fpr_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vals['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_done = evaluate_pipeline(p1, X_train, y_train, X_test, y_test, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr1, tpr1, label='Group 1')\n",
    "plt.plot(fpr2, tpr2, label='Group 2')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('------------BEFORE BIAS ANALYSIS------------')\n",
    "\n",
    "statistical_parity = np.sum(preds_s1)/len(s1) - np.sum(preds_s2)/len(s2)\n",
    "print('Statistical parity: ', (statistical_parity))\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "\n",
    "best_vals, balls1, balls2 = minimize_odd_diff(true_s1=true_s1, true_s2=true_s2, scores_1= scores_s1,scores_2= scores_s2, k= 1000)\n",
    "\n",
    "preds_s1 = scores_s1 >= best_vals['threshold']\n",
    "preds_s2 = scores_s2 >= best_vals['threshold']\n",
    "\n",
    "# equalized odds (horizontal)\n",
    "tpr1 = tp1 / (tp1 + fn1)\n",
    "fpr1 = fp1 / (fp1 + tn1)\n",
    "tpr2 = tp2 / (tp2 + fn2)\n",
    "fpr2 = fp2 / (fp2 + tn2)\n",
    "\n",
    "print('FPR diff: ', abs(fpr1 - fpr2))\n",
    "print('TPR Diff', abs(tpr1 - tpr2))\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "print('positive outcome:', abs(some_1 - some_2))\n",
    "print('negative outcome:', abs(some_3 - some_4))\n",
    "\n",
    "\n",
    "print('------------AFTER BIAS ANALYSIS------------')\n",
    "\n",
    "statistical_parity = np.sum(preds_s1)/len(s1) - np.sum(preds_s2)/len(s2)\n",
    "print('Statistical parity: ', (statistical_parity))\n",
    "\n",
    "tn1, fp1, fn1, tp1 = confusion_matrix(preds_s1, true_s1).ravel()\n",
    "fpr_group1 = fp1 / (fp1 + tn1)\n",
    "tpr_group1 = tp1 / (tp1 + fn1)\n",
    "\n",
    "tn2, fp2, fn2, tp2 = confusion_matrix(preds_s2, true_s2).ravel()\n",
    "fpr_group2 = fp2 / (fp2 + tn2)\n",
    "tpr_group2 = tp2 / (tp2 + fn2)\n",
    "\n",
    "# equalized odds (horizontal)\n",
    "print('FPR diff: ', abs(fpr_group1 - fpr_group2))\n",
    "print('TPR Diff', abs(tpr_group1 - tpr_group2))\n",
    "\n",
    "# equalized outcomes, given selection, groups have equal outcomes\n",
    "some_1 = tp1 / (tp1 + fp1)\n",
    "some_2 = tp2 / (tp2 + fp2)\n",
    "some_3 = tn1 / (tn1 + fn1)\n",
    "some_4 = tn2 / (tn2 + fn2)\n",
    "\n",
    "print('positive outcome diff:', abs(some_1 - some_2))\n",
    "print('negative outcome diff:', abs(some_3 - some_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_vals['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.plot(np.linspace(0, 1, 1000),balls1, label = 'tpr_diff')\n",
    "plt.plot(np.linspace(0, 1, 1000), balls2, label = 'fpr_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes to be made to models\n",
    "The accuracy of the classifier sits at 0.77 as of baseline test with the selected pipeline. Standard-scaling was used across all features, which may end up causing trouble. The model has a fairly high accuracy for a baseline classifier, with desirable f1-scores at [0.72, 0.81]. Since the outcome variable in this case is almost equally balanced at the chosen threshold, we need not to scale our model much with respect to the outcome variable. In this pipeline discrete variables are scaled, which changes the otherwise explainable binary variables, which hurts the explainability of the model. <br>\n",
    "<br>\n",
    "We would need to only scale the age column in the data to fix this explainability issue. Unless we had more continuous ways of representing the discrete variables in the data (such as language test scores for english proficiency, SAT scores for education etc.) we would not be able to quantify these columns in any meaningful fashion.\n",
    "\n",
    "### What we did\n",
    "In trying to improve the interpretability of our model, we limit the scaling to exclusively be applied to the \"AGEP\" column, seeing as it is the only continuous variable in the data. We also use the template's onehot-encodings applied to the discrete variables, with dropping the first value applied. This way we can compare each categorical variable to the baseline of the one which is dropped. <br>\n",
    "\n",
    "Before making any changes to any of the models we find the following accuracies of the classifiers: \n",
    "<br>\n",
    "(insert classification report for models)\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores affected by Equalized outcome\n",
    "In fine-tuning the thresholds for our models to obtain statistical parity, we found that we had to sacrifice model accuracy on Group 1 (Males), to equalize the outcome accuracy. This meant we had to choose a suboptimal threshold for Group 1 and chose the best Threshold for Group 2. When applying this to the logistic regression model, we found that changing the thresholds obtained equalized outcomes within ~1 percent difference, this threshold change also moved the model closer to obtaining equalized odds, with only a ~2 percent difference. <br>\n",
    "<br>\n",
    "When applying this method to the Blackbox Random Forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model agnostic explainations with SHAP\n",
    "\n",
    "Here we use model agnostic explainations with SHAP to explain both of the models. To summarize the sections are:\n",
    "\n",
    " - Row chosen for comparison and printed\n",
    " - For Logistic Regression and then Random Forest after:\n",
    "    1. Calculate SHAP values\n",
    "    2. Generate bar plot\n",
    "    3. Generate beeswarm plot\n",
    "    4. Generate force plot for specific row\n",
    "    5. Generate force plot for specific row with aggregated one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rows chosen for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ended up just choosing row 0 for simplicity - so we have a legacy naming convention\n",
    "specific_rows = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating SHAP values...')\n",
    "X_train_ = X_train.to_numpy().astype(float)\n",
    "X_test_ = X_test.to_numpy().astype(float)\n",
    "lr_explainer = shap.Explainer(\n",
    "    p2,\n",
    "    X_train_,\n",
    "    # model_output='probability',\n",
    "    # link=shap.links.logit,\n",
    "    # algorithm='linear',\n",
    "    feature_names=X_train.columns\n",
    "    )\n",
    "\n",
    "lr_shap_values = lr_explainer(X_test_)\n",
    "\n",
    "print('Generating general bar plot...')\n",
    "shap.plots.bar(lr_shap_values, max_display=8)\n",
    "\n",
    "print('Generating general beeswarm plot...')\n",
    "shap.plots.beeswarm(lr_shap_values, max_display=8, show=False)\n",
    "ax = plt.gca()\n",
    "# You can change the min and max value of xaxis by changing the arguments of:\n",
    "ax.set_xlim(-3.5, 3.5) \n",
    "plt.show()\n",
    "\n",
    "print('Generating row specific plots...')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    # shap.force_plot(shap_values[i], link='logit', matplotlib=True)\n",
    "    shap.force_plot(lr_shap_values[i], matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_idx = list()\n",
    "shap_agg_full = np.zeros((lr_shap_values.shape[0], len(X_train.columns)))\n",
    "for i in feature_names:\n",
    "    group_idx.append([n for n, l in enumerate(X_train.columns) if l.startswith(i)])\n",
    "\n",
    "# group_idx now contains the indices of the features that belong to each group\n",
    "# we can now sum the SHAP values for each group\n",
    "    \n",
    "for ii, g in enumerate(group_idx):\n",
    "    shap_agg_full[:, ii] = np.sum(lr_shap_values[:, g].values, -1)\n",
    "\n",
    "# copy and replace? - YES\n",
    "lr_shap_values_copy = lr_shap_values\n",
    "lr_shap_values_copy.values = shap_agg_full\n",
    "\n",
    "# now can do the force plots again\n",
    "print('Generating row specific plots with aggregated categories...')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    # shap.force_plot(shap_values[i], link='logit', matplotlib=True)\n",
    "    shap.force_plot(lr_shap_values_copy[i], feature_names=feature_names, matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating SHAP values... can take up to 15 mins...')\n",
    "X_train_ = X_train.to_numpy().astype(float)\n",
    "X_test_ = X_test.to_numpy().astype(float)\n",
    "rf_explainer = shap.Explainer(p1, X_train_, model_output='probability', feature_names=X_train.columns)\n",
    "rf_shap_values = rf_explainer(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating general bar plot...')\n",
    "shap.plots.bar(rf_shap_values[:,:,1], max_display=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating general beeswarm plot...')\n",
    "shap.plots.beeswarm(rf_shap_values[:,:,1], max_display=8)\n",
    "\n",
    "# unused scaling:\n",
    "# ax = plt.gca()\n",
    "# # You can change the min and max value of xaxis by changing the arguments of:\n",
    "# ax.set_xlim(-3.5, 3.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating row specific plots...')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    row_to_explain = X_test_[i]\n",
    "    row_shap_values = rf_explainer(row_to_explain)\n",
    "    shap.plots.force(rf_explainer.expected_value[1], row_shap_values.values[:,1], row_to_explain, feature_names=X_train.columns, matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating row specific plots with aggregated categories...')\n",
    "for i in specific_rows:\n",
    "    print(f'Row {i}')\n",
    "    \n",
    "    row_to_explain = X_test_[i]\n",
    "    row_shap_values = rf_explainer(row_to_explain)\n",
    "    \n",
    "    group_idx = list()\n",
    "    shap_agg_row = np.zeros((row_shap_values.shape[0], 2))\n",
    "    \n",
    "    for i in feature_names:\n",
    "        group_idx.append([n for n, l in enumerate(X_train.columns) if l.startswith(i)])\n",
    "\n",
    "    for idx, g in enumerate(group_idx):\n",
    "        shap_agg_row[idx, 1] = np.sum(row_shap_values[g, 1].values, -1)\n",
    "\n",
    "    row_shap_values_copy = row_shap_values\n",
    "    row_shap_values_copy.values = shap_agg_row\n",
    "\n",
    "    shap.plots.force(rf_explainer.expected_value[1], row_shap_values.values[:,1], row_to_explain, feature_names=feature_names, matplotlib=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
